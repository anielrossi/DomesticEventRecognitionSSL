{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "not yet implemented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm, model_selection\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import normalize\n",
    "import json\n",
    "import itertools\n",
    "import utils\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def svc_param_selection(X, y, nfolds):\n",
    "    Cs = [0.001, 0.01, 0.1, 1, 10]\n",
    "    gammas = [0.001, 0.01, 0.1, 1]\n",
    "    param_grid = {'C': Cs, 'gamma' : gammas}\n",
    "    grid_search = model_selection.GridSearchCV(svm.SVC(kernel='linear'), param_grid, cv=nfolds)\n",
    "    grid_search.fit(X, y)\n",
    "    grid_search.best_params_\n",
    "    return grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded dataset with 13 classes:\n",
      "\tCoin (dropping): 127 sounds\n",
      "\tCupboard open or close: 104 sounds\n",
      "\tDoor: 155 sounds\n",
      "\tDrawer open or close: 117 sounds\n",
      "\tKeys jangling: 122 sounds\n",
      "\tMicrowave oven: 98 sounds\n",
      "\tPacking tape, duct tape: 95 sounds\n",
      "\tScissors: 126 sounds\n",
      "\tToilet flush: 156 sounds\n",
      "\tTyping: 153 sounds\n",
      "\tVacuum cleaner: 102 sounds\n",
      "\tWriting: 147 sounds\n",
      "\tZipper (clothing): 132 sounds\n"
     ]
    }
   ],
   "source": [
    "DATASET_NAME = 'baseline_candidates'\n",
    "true_dataset = utils.load_from_json('%s.json' % DATASET_NAME)\n",
    "print('Loaded dataset with %i classes:' % len(true_dataset))\n",
    "for klass, sounds in true_dataset.items():\n",
    "    print('\\t%s: %i sounds' % (klass, len(sounds)))\n",
    "class_names = list(true_dataset.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Coin (dropping)',\n",
       " 'Cupboard open or close',\n",
       " 'Door',\n",
       " 'Drawer open or close',\n",
       " 'Keys jangling',\n",
       " 'Microwave oven',\n",
       " 'Packing tape, duct tape',\n",
       " 'Scissors',\n",
       " 'Toilet flush',\n",
       " 'Typing',\n",
       " 'Vacuum cleaner',\n",
       " 'Writing',\n",
       " 'Zipper (clothing)']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CODE TO CLEAN DATASET (remove sounds which do not have analysis information)\n",
    "dataset_cleaned = {}\n",
    "for key, sounds in true_dataset.items():\n",
    "    if key not in dataset_cleaned:\n",
    "        dataset_cleaned[key] = []\n",
    "    for sound in sounds:\n",
    "        if sound['analysis'] is not None:\n",
    "            dataset_cleaned[key].append(sound)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_feature_vector(sound):\n",
    "    features = []\n",
    "    #add control if they have it or not\n",
    "    if (sound['analysis']):\n",
    "        mfcc_1 = sound['analysis']['lowlevel']['mfcc']['mean'][1]\n",
    "        mfcc_1_1 = sound['analysis']['lowlevel']['mfcc']['dmean'][1]\n",
    "        mfcc_2 = sound['analysis']['lowlevel']['mfcc']['mean'][2]\n",
    "        mfcc_2_1 = sound['analysis']['lowlevel']['mfcc']['dmean'][2]\n",
    "        mfcc_3 = sound['analysis']['lowlevel']['mfcc']['mean'][3]\n",
    "        mfcc_3_1 = sound['analysis']['lowlevel']['mfcc']['dmean'][3]\n",
    "        mfcc_4 = sound['analysis']['lowlevel']['mfcc']['mean'][4]\n",
    "        mfcc_4_1 = sound['analysis']['lowlevel']['mfcc']['dmean'][4]\n",
    "        mfcc_5 = sound['analysis']['lowlevel']['mfcc']['mean'][5]\n",
    "        mfcc_5_1 = sound['analysis']['lowlevel']['mfcc']['dmean'][5]\n",
    "        mfcc_6 = sound['analysis']['lowlevel']['mfcc']['mean'][6]\n",
    "        mfcc_6_1 = sound['analysis']['lowlevel']['mfcc']['dmean'][6]\n",
    "        mfcc_7 = sound['analysis']['lowlevel']['mfcc']['mean'][7]\n",
    "        mfcc_7_1 = sound['analysis']['lowlevel']['mfcc']['dmean'][7]\n",
    "        mfcc_8 = sound['analysis']['lowlevel']['mfcc']['mean'][8]\n",
    "        mfcc_8_1 = sound['analysis']['lowlevel']['mfcc']['dmean'][8]\n",
    "        mfcc_9 = sound['analysis']['lowlevel']['mfcc']['mean'][9]\n",
    "        mfcc_9_1 = sound['analysis']['lowlevel']['mfcc']['dmean'][9]\n",
    "        mfcc_10 = sound['analysis']['lowlevel']['mfcc']['mean'][10]\n",
    "        mfcc_10_1 = sound['analysis']['lowlevel']['mfcc']['dmean'][10]\n",
    "        mfcc_11 = sound['analysis']['lowlevel']['mfcc']['mean'][11]\n",
    "        mfcc_11_1 = sound['analysis']['lowlevel']['mfcc']['dmean'][11]\n",
    "        mfcc_12 = sound['analysis']['lowlevel']['mfcc']['mean'][12]\n",
    "        mfcc_12_1 = sound['analysis']['lowlevel']['mfcc']['dmean'][12]\n",
    "\n",
    "        #feature vector creation\n",
    "        features = np.concatenate([[], features, np.array([mfcc_1])])\n",
    "        features = np.concatenate([[], features, np.array([mfcc_1_1])])\n",
    "        features = np.concatenate([[], features, np.array([mfcc_2])])\n",
    "        features = np.concatenate([[], features, np.array([mfcc_2_1])])\n",
    "        features = np.concatenate([[], features, np.array([mfcc_3])])\n",
    "        features = np.concatenate([[], features, np.array([mfcc_3_1])])\n",
    "        features = np.concatenate([[], features, np.array([mfcc_4])])\n",
    "        features = np.concatenate([[], features, np.array([mfcc_4_1])])\n",
    "        features = np.concatenate([[], features, np.array([mfcc_5])])\n",
    "        features = np.concatenate([[], features, np.array([mfcc_5_1])])\n",
    "        features = np.concatenate([[], features, np.array([mfcc_6])])\n",
    "        features = np.concatenate([[], features, np.array([mfcc_6_1])])\n",
    "        features = np.concatenate([[], features, np.array([mfcc_7])])\n",
    "        features = np.concatenate([[], features, np.array([mfcc_7_1])])\n",
    "        features = np.concatenate([[], features, np.array([mfcc_8])])\n",
    "        features = np.concatenate([[], features, np.array([mfcc_8_1])])\n",
    "        features = np.concatenate([[], features, np.array([mfcc_9])])\n",
    "        features = np.concatenate([[], features, np.array([mfcc_9_1])])\n",
    "        features = np.concatenate([[], features, np.array([mfcc_10])])\n",
    "        features = np.concatenate([[], features, np.array([mfcc_10_1])])\n",
    "        features = np.concatenate([[], features, np.array([mfcc_11])])\n",
    "        features = np.concatenate([[], features, np.array([mfcc_11_1])])\n",
    "        features = np.concatenate([[], features, np.array([mfcc_12])])\n",
    "        features = np.concatenate([[], features, np.array([mfcc_12_1])])\n",
    "    else:\n",
    "        print(str(sound['id']) + \": has no analysis\")\n",
    "        #this is wrong but should never happen because we clean the dataset(only for debug)\n",
    "        features = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for fitting classifier (as sklearn classifiers require)\n",
    "y = list()\n",
    "X = list()\n",
    "for class_name, sounds in dataset_cleaned.items():\n",
    "    for count, sound in enumerate(sounds):\n",
    "        # Use index of class name in class_names as numerical value (classifier internally represents\n",
    "        # class label as number)\n",
    "        y.append(class_names.index(class_name))\n",
    "        feature_vector = build_feature_vector(sound)\n",
    "        X.append(np.array(feature_vector))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalize features\n",
    "X_normalized = normalize(X)\n",
    "\n",
    "#with normalization it performs worse!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_param_selection(X, y, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = svm.SVC(C=10, gamma=0.001)\n",
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metrics.accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute confusion matrix\n",
    "cnf_matrix = metrics.confusion_matrix(y_test, y_pred)\n",
    "np.set_printoptions(precision=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names_short = ['coi', 'cpb', 'dor', 'drw', 'key', 'mrw', 'pck', 'sci', 'tof', 'typ', 'vcc', 'wrt', 'zip']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot non-normalized confusion matrix\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix, classes=class_names_short,\n",
    "                      title='Confusion matrix, without normalization')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#report = metrics.classification_report(y_test, y_pred, target_names=class_names_short)\n",
    "report = metrics.classification_report(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classification_report_csv(report):\n",
    "    report_data = []\n",
    "    lines = report.split('\\n')\n",
    "    for line in lines[2:-3]:\n",
    "        row = {}\n",
    "        row_data = line.split('      ')\n",
    "        row['class'] = row_data[0]\n",
    "        row['precision'] = float(row_data[1])\n",
    "        row['recall'] = float(row_data[2])\n",
    "        row['f1_score'] = float(row_data[3])\n",
    "        row['support'] = float(row_data[4])\n",
    "        report_data.append(row)\n",
    "    dataframe = pd.DataFrame.from_dict(report_data)\n",
    "    dataframe.to_csv('classification_report.csv', index = False)\n",
    "\n",
    "classification_report_csv(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
